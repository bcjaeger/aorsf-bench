---
title: "Accelerated oblique random survival forests"
author: "Byron C. Jaeger"
date: Last updated `r Sys.Date()`
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}

options(htmltools.dir.version = FALSE)

knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  dpi = 600,
  fig.align='center',
  fig.height=5
)

library(targets)
library(tidyverse)
library(ggforce)
library(rpart)
library(rpart.plot)
library(party)
library(parttree)
library(palmerpenguins)
library(gt)
library(table.glue)

tar_load(
  names = c(
    bm_pred_model_viz,
    bm_pred_time_viz,
    bm_pred_viz,
    bm_vi_smry,
    data_key
  )
)

```

```{css texts}

.medium { font-size: 25px; }
.mediumer { font-size: 40px; }
.large { font-size: 80px; }
.larger { font-size: 160px; }
.huge { font-size: 240px; }

.rainbow-text {
  background-image: linear-gradient(to left, violet, indigo, blue, green, yellow, orange, red);   -webkit-background-clip: text;
  color: transparent;
}

```

## Overview

- Random forests (axis based and oblique)

- Accelerating the oblique RSF

- Benchmark of prediction accuracy

- Benchmark of variable importance

- R package & website

---
class: center, middle, inverse

# .larger[Random forests] 
## (axis based and oblique)

---
background-image: url(img/penguins.png)
background-size: 45%
background-position: 85% 72.5%

## Decision trees

- Frequently used in supervised learning.

- Partitions the space of predictor variables.

- Can be used for classification, regression, and survival analysis. 

.pull-left[
.medium[_Demo_:] <br><br> Axis-based and oblique decision trees for classification of penguin species (chinstrap, gentoo, or adelie) based on bill and flipper length.<sup>1</sup>
]

.footnote[
<sup>1</sup>Data were collected and made available by [Dr. Kristen Gorman](https://www.uaf.edu/cfos/people/faculty/detail/kristen-gorman.php) and the [Palmer Station](https://pal.lternet.edu/), a member of the [Long Term Ecological Research Network](https://lternet.edu/).
]

---

Dimensions for Adelie, Chinstrap and Gentoo Penguins at Palmer Station

```{r fig_penguins_nopart}

penguins <- drop_na(penguins)

ggplot(data = penguins) +
  aes(x = flipper_length_mm, y = bill_length_mm, label = species) +
  geom_point(aes(color = species, shape = species),
             size = 3,
             alpha = 0.8) +
  geom_mark_ellipse(aes(color = species, fill = species), alpha = 0.075) +
  theme_minimal() +
  scale_color_manual(values = c("darkorange","purple","cyan4")) +
  scale_fill_manual(values = c("darkorange","purple","cyan4")) +
  labs(x = "\nFlipper length, mm",
       y = "Bill length, mm\n",
       color = "Penguin species",
       shape = "Penguin species") +
  coord_cartesian(ylim = c(30, 70),
                  xlim = c(170, 235)) +
  theme(panel.grid = element_blank(),
        legend.position = '')

```

---

Partition all the penguins into flipper length < 207 or â‰¥ 207 mm

```{r fig_penguins_part1}

mdl_tree <- rpart(
 formula = species ~ flipper_length_mm + bill_length_mm,
 data = penguins, 
 control = rpart.control(maxdepth = 1)
)


ggplot(data = penguins) +
  aes(x = flipper_length_mm, y = bill_length_mm, label = species) +
  geom_point(aes(color = species, shape = species),
             size = 3,
             alpha = 0.8) +
  geom_parttree(data = mdl_tree, aes(fill=species), alpha = 0.1) +
  theme_minimal() +
  scale_color_manual(values = c("darkorange","purple","cyan4")) +
  scale_fill_manual(values = c("darkorange","purple","cyan4")) +
  labs(x = "\nFlipper length, mm",
       y = "Bill length, mm\n",
       color = "Penguin species",
       shape = "Penguin species") +
  coord_cartesian(ylim = c(30, 70),
                  xlim = c(170, 235)) +
  scale_x_continuous(expand = c(0,0), breaks = c(180, 200, 220)) + 
  scale_y_continuous(expand = c(0,0)) +
  theme(panel.grid = element_blank(),
        legend.position = '')


```

---

Partition penguins on the left side into into bill length < 43 or â‰¥ 43 mm

```{r fig_penguins_part2}

mdl_tree <- rpart(
 formula = species ~ flipper_length_mm + bill_length_mm,
 data = penguins, 
 control = rpart.control(maxdepth = 2)
)

ggplot(data = penguins) +
  aes(x = flipper_length_mm, y = bill_length_mm, label = species) +
  geom_point(aes(color = species, shape = species),
             size = 3,
             alpha = 0.8) +
  geom_parttree(data = mdl_tree, aes(fill=species), alpha = 0.1) +
  theme_minimal() +
  scale_color_manual(values = c("darkorange","purple","cyan4")) +
  scale_fill_manual(values = c("darkorange","purple","cyan4")) +
  labs(x = "\nFlipper length, mm",
       y = "Bill length, mm\n",
       color = "Penguin species",
       shape = "Penguin species") +
  coord_cartesian(ylim = c(30, 70),
                  xlim = c(170, 235)) +
  scale_x_continuous(expand = c(0,0), breaks = c(180, 200, 220)) + 
  scale_y_continuous(expand = c(0,0)) +
  theme(panel.grid = element_blank(),
        legend.position = '')


# ---
# background-image: url(img/penguins_logo.png)
# background-position: 95% 5%
# background-size: 110px 120px
# layout: true

```

---

With oblique splits, partitions do not need to be rectangles

```{r fig_penguins_part2_oblique}

x_delta <- 206.5 - 170
y_delta <- 46 - 40

slope <- y_delta / x_delta
intercept <- 46 - 206.5 * slope

text_lower <-
  table_glue("bill length - {slope} * flipper length < {intercept}")
text_upper <-
  table_glue("bill length - {slope} * flipper length \u2265 {intercept}")

data_poly <- tribble(
  ~flipper_length_mm, ~bill_length_mm    , ~species,
  170               ,  30                , 'Adelie',
  206.5             ,  30                , 'Adelie',
  206.5             ,  46                , 'Adelie',
  170               ,  40                , 'Adelie',
  170               ,  40                , 'Chinstrap',
  206.5             ,  46                , 'Chinstrap',
  206.5             ,  70                , 'Chinstrap',
  170               ,  70                , 'Chinstrap',
  206.5             ,  30                , 'Gentoo',
  235               ,  30                , 'Gentoo',
  235               ,  70                , 'Gentoo',
  206.5             ,  70                , 'Gentoo',
)

ggplot(data = penguins) +
  aes(x = flipper_length_mm, y = bill_length_mm, label = species) +
  geom_polygon(data = data_poly, 
               aes(fill = species, group = species),
               alpha = 0.1,
               col = 'black') +
  geom_point(aes(color = species, shape = species),
             size = 3,
             alpha = 0.8) +
  # geom_abline(slope = slope, intercept = intercept, col = 'red')
  geom_text(
    data = tibble(flipper_length_mm = c(172, 172), 
                  bill_length_mm = c(68, 32), 
                  species = c(text_upper, text_lower)),
    hjust = 0
  ) + 
  theme_minimal() +
  scale_color_manual(values = c("darkorange","purple","cyan4")) +
  scale_fill_manual(values = c("darkorange","purple","cyan4")) +
  labs(x = "\nFlipper length, mm",
       y = "Bill length, mm\n",
       color = "Penguin species",
       shape = "Penguin species") +
  coord_cartesian(ylim = c(30, 70),
                  xlim = c(170, 235)) +
  scale_x_continuous(expand = c(0,0), breaks = c(180, 200, 220)) + 
  scale_y_continuous(expand = c(0,0)) +
  theme(panel.grid = element_blank(),
        legend.position = '')

```

---

## Random survival forests (RSFs)

1. Breiman developed the random forest, a large set of decision trees injected with randomness.<sup>1, 2</sup>

.footnote[<sup>1</sup>Breiman, Leo. "Bagging predictors." Machine learning 24.2 (1996): 123-140.<br/><sup>2</sup>Breiman, Leo. "Random forests." Machine learning 45.1 (2001): 5-32.]

---

## Random survival forests (RSFs)

1. Breiman developed the random forest, a large set of decision trees injected with randomness.<sup>1, 2</sup>

1. Hothorn and, separately, Ishwaran developed extensions of the random forest for survival outcomes.<sup>3, 4</sup>

Specifically, 

- Hothorn developed the conditional inference forest (CIF) 

- Ishwaran developed the random survival forest (RSF)

.footnote[<sup>3</sup>Hothorn, Torsten, et al. "Unbiased recursive partitioning: A conditional inference framework." Journal of Computational and Graphical statistics 15.3 (2006): 651-674.<br/><sup>4</sup> Ishwaran, Hemant, et al. "Random survival forests." Annals of Applied Statistics 2.3 (2008): 841-860.]

---

Each leaf in the RSF contains a Kaplan-Meier estimate of survival. In the CIF, weights are applied based on sample size.

```{r}

penguins_sim <- penguins %>%
  mutate(
    time_mean = if_else(
      bill_length_mm < 40,
      true = 80,
      false = 20
    ),
    time = rnorm(n = n(), mean = time_mean, sd = 20),
    time = pmax(time, 1),
    status = rbinom(n = n(), size = 1, prob = 0.75)
  )

# png(res = 300,
#     width = 6,
#     height = 3.75,
#     units = 'in',
#     filename = 'rpart_plot_surv.png')

mdl_ctree <-
  ctree(formula = Surv(time, status) ~ bill_length_mm,
        data = penguins_sim,
        controls = ctree_control(maxdepth = 1))

plot(
  mdl_ctree,
  inner_panel = node_inner(mdl_ctree, pval = FALSE)
)

```

---

## Random survival forests (RSFs)

1. Breiman developed the random forest, a large set of decision trees injected with randomness.<sup>1, 2</sup>

1. Hothorn and, separately, Ishwaran developed extensions of the random forest to engage with survival outcomes (CIF and RSF).<sup>3, 4</sup>

1. Zhou developed a rotation survival forest and Wang developed a survival forest with an extended predictor space.<sup>5, 6</sup>

Both Zhou and Wang's extensions were based on the CIF


.footnote[<sup>5</sup>Zhou L, et al. "Rotation survival forest for right censored data." PeerJ. 2015 Jun 11;3:e1009.<br/><sup>6</sup> Wang H, et al. "Random survival forest with space extensions for censored data." Artif Intell Med. 2017 Jun;79:52-61.]

---

## Random survival forests (RSFs)

1. Breiman developed the random forest, a large set of decision trees injected with randomness.<sup>1, 2</sup>

1. Hothorn and, separately, Ishwaran developed extensions of the random forest to engage with survival outcomes (CIF and RSF).<sup>3, 4</sup>

1. Zhou developed a rotation survival forest and Wang developed a survival forest with an extended predictor based on the CIF.<sup>5, 6</sup>

1. Jaeger developed the oblique RSF, which used penalized regression to find oblique splits.<sup>7</sup>

Jaeger showed in general benchmarks that the oblique RSF had higher prediction accuracy than axis-based RSFs. However, the obliqueRSF R package is hundreds of times slower than standard R packages for axis based RSFs.

.footnote[<sup>7</sup>Jaeger BC, et al. "Oblique random survival forests." The Annals of Applied Statistics 13.3 (2019): 1847-1883.]

---
class: center, middle
background-image: url("img/meme_slow_R.jpg")
background-size: contain

---
class: center, middle, inverse

# .larger[Accelerating the oblique RSF]

---


## Accelerating the oblique RSF

We identify linear combinations of predictor variables in non-leaf nodes by applying Newton Raphson scoring to the partial likelihood function of the Cox regression model: $$L(\beta) = \prod_{i=1}^m \frac{e^{x_{j(i)}^{'} \beta}}{\sum_{j \in R_i} e^{x_j^{'} \beta}}$$

- $x_i$ is a vector of predictors values.

- $R_i$ is the set of indices, $j$, with $T_j \geq t_i$ (i.e., those still at risk at time $t_i$)

    + $T_i$ is the event time if an event occurred and last point of contact otherwise.

    + $t_1 < \, \ldots \, < t_m$ are the $m$ unique event times in the training data.

- $j(i)$ is the index of the observation for which an event occurred at time $t_i$.

---

## Newton Raphson scoring

Estimated regression coefficients $\hat{\beta}$ are updated in each step based on their first derivative, $U(\hat{\beta})$, and second derivative, $H(\hat{\beta})$:
$$\hat{\beta}^{k+1} =  \hat{\beta}^{k} + U(\hat{\beta} = \hat{\beta}^{k})\, H^{-1}(\hat{\beta} = \hat{\beta}^{k})$$

For statistical inference, iterate until a convergence threshold is met. 

For identifying linear combination of predictors in the oblique RSF, .mediumer[ðŸ¤·]

- `aorsf-fast` completes one iteration.

- `aorsf-cph` iterates until convergence or 15 iterations.

---

## Negation variable importance (VI)

Negation VI is similar to permutation VI in that it measures how much a modelâ€™s prediction error increases when a variableâ€™s role in the model is de-stabilized. 

- Negation VI measures the increase in an oblique RF's prediction error after flipping the sign of all coefficients linked to a variable. 

- As the magnitude of a coefficient increases, so does the probability that negating it will change the oblique RF's predictions. 

- Since the coefficients in each non-leaf node of an oblique RFs are adjusted for the accompanying predictors, negation VI may provide better estimation of VI in the presence of correlated variables compared to standard VI techniques.

---
class: center, middle, inverse

# .larger[Prediction Benchmark]

---

## Learners

__Oblique RSFs__:

- _aorsf-fast_: the fast version of aorsf

- _aorsf-cph_: the less fast but still pretty fast version of aorsf

- _aorsf-random_: randomized coefficients (Breiman's idea)

- _aorsf-net_: aorsf's copy of obliqueRSF

- _obliqueRSF-net_: oblique RSF using penalized cox regression (the original)

---

## Learners

__Oblique RSFs__:

- .rainbow-text[_aorsf-fast_]: this is the only oblique RSF we show in the racing model plots.

- _aorsf-cph_: the less fast but still pretty fast version of aorsf

- _aorsf-random_: randomized coefficients (Breiman's idea)

- _aorsf-net_: aorsf's copy of obliqueRSF

- _obliqueRSF-net_: oblique RSF using penalized cox regression (the original)

---

## Learners

__Axis based RSFs__:

- _cif-standard_: standard CIF

- _cif-extension_: CIF with space extension

- _cif-rotate_: CIF with rotation

- _rsf-standard_: standard RSF

- _ranger-extratrees_: RSF with extremely randomized trees

---

## Learners

__Other__:

- _glmnet-cox_: Penalized Cox regression model

- _nn-cox_: Cox neural network with time-varying effects

- _xgboost-cox_: Boosted trees fitted to Cox log likelihood

- _xgboost-aft_: Boosted trees fitted to accelerated failure time.

---

## Data sets

A total of 31 risk prediction tasks in 18 data sets were analyzed.

- number of observations ranged from `r min(data_key$nrow)` to `r max(data_key$nrow)` (median = `r median(data_key$nrow)`)

- number of predictors ranged from `r min(data_key$ncol)` to `r max(data_key$ncol)` (median = `r median(data_key$ncol)`)

- % censored ranged from `r round(100*min(data_key$pcens))` to `r round(100*max(data_key$pcens))` (median = `r round(100*median(data_key$pcens))`)

(Full table is shown in the bonus slides)

---

## Evaluation

We measured performance of each learner with:

- index of prediction accuracy (IPA); __higher__ is ðŸ‘ 

- time-dependent concordance (C)-statistic; __higher__ is ðŸ‘

- total time to fit a model and compute predictions; __lower__ is ðŸ‘

---

## Evaluation

To estimate overall performance differences:

Step 1: Collect IPA and C-statistic values:

- For each of the 31 risk prediction problems,

    + split the corresponding data into a 50/50 train/test split
    + fit each learner to the training set
    + evaluate each learner's predictions in the testing set
    + repeat 25 times

--

Step 2: Fit a Bayesian linear mixed model to analyze posterior expected differences in performance:

$$\text{metric} = \widehat{\gamma} \cdot \text{learner} + (1\,|\, \text{data/run})$$

- `run` refers to the specific train/test split of `data` 

- `metric` is either the IPA or the time-dependent C-statistic.



---

```{r bm_pred_viz_ibs, fig.height=9, fig.width=11}
bm_pred_viz$ibs_scaled
```

---

```{r bm_pred_viz_cstat, fig.height=9, fig.width=11}
bm_pred_viz$cstat
```

---

```{r bm_pred_model_viz_ibs, fig.height=6.9, fig.width=9}
bm_pred_model_viz$ibs_scaled
```

---

```{r bm_pred_model_viz_cstat, fig.height=6.9, fig.width=9}
bm_pred_model_viz$cstat
```

---

```{r bm_pred_time_ridges, fig.height=6.5, fig.width=8}
bm_pred_time_viz
```

---
class: center, middle, inverse

# .larger[Variable Importance Benchmark]

---

## Variable importance techniques

- `randomForestSRC-permutation` - the classic approach proposed by Leo Breiman

- `aorsf-negation` - the new approach I am proposing

- `aorsf-anova` - The only (?) other variable importance technique for oblique random forests

- `aorsf-shap` - SHAP values from an accelerated oblique RSF model

- `xgboost-shap` - SHAP values from an xgboost model (gold standard)

- `xgboost-gain` - Gain importance from an xgboost model (classic)

---

## Simulated data

We simulated five classes of predictor variables:

- __junk__: no relationship with the outcome.

- __main__: linearly related to the outcome

- __non-linear__: sinusoidal relationship to the outcome

- __combination__: latent variables formed from combinations of the combination variables have a linear relationship to the outcome.

- __interactions__: related to the outcome by multiplicative interaction with one other variable.

---

## Simulated data

Start with a multivariate normal draw

- All columns have a mean of zero and standard deviation of 1. 

- Pairwise correlations are selected at random uniformly between a lower and upper boundary. 

- A predictor matrix including interactions, non-linear mappings, and linear combinations is used to generate a time-to-event outcome using the `simsurv` R package

- Transforms are dropped from the matrix after generating the outcome values.

---

## Evaluation

VI techniques are compared based on their discrimination (C-statistic) between relevant and irrelevant variables. 

Normally a C-statistic assesses if a predicted score discriminates between positive and negative cases of a binary outcome. 

We use the C-statistic differently by treating each variable like a binary outcome (e.g., the outcome is 1 if the variable is relevant, 0 otherwise).

In other words, we assess VI as if it were a â€˜predictionâ€™ for the status of the variable. (can only do this for simulated data).

The resulting C-statistic measures the probability that a VI technique will rank a relevant variable higher than an irrelevant variable.

---

```{r out.width='100%', fig.align='center'}

library(plotly)

fig_data <- bm_vi_smry %>%
  ungroup() %>%
  filter(n_obs == 1000) %>%
  mutate(
    model = recode(
      model, 'randomForestSRC-permutation' = 'rfsrc-permutation'
    ),
    model = fct_reorder(model, .x = overall),
    pred_corr_max = factor(pred_corr_max),
    text = table_glue(
      "{model}<br>\\
          Mean computation time: {as.numeric(time)} seconds<br>\\
          C-statistic by variable class:<br>\\
          Main effect: {main}<br>\\
          Non-linear effect: {nlin}<br>\\
          Combination effect: {cmbn}<br>\\
          Interactions:<br>\\
             w/main effect: {intr_main}<br>\\
             w/non-linear effect: {intr_hidden_nlin}<br>\\
             w/combination effect: {intr_hidden_cmbn}"
    ))

fig <- plot_ly(width = 800, height = 650) %>%
  add_trace(
    type = 'bar',
    x = fig_data$pred_corr_max,
    y = fig_data$overall,
    color = fig_data$model,
    colors = "PuRd",
    text = table_value(fig_data$overall),
    textposition = 'outside',
    hoverinfo = 'text',
    hovertext = fig_data$text
  ) %>%
  plotly::layout(
    title = list(text = 'Results when N = 1000', x = 0.1),
    xaxis = list(title = 'Max correlation among predictors'),
    yaxis = list(title = 'C-statistic', range = c(0,1)),
    legend = list(title = list(text = '<b>VI technique</b>'))
  )

fig


```


---


```{r out.width='100%', fig.align='center'}

library(plotly)

fig_data <- bm_vi_smry %>%
  ungroup() %>%
  filter(n_obs == 5000) %>%
  mutate(
    model = recode(
      model, 'randomForestSRC-permutation' = 'rfsrc-permutation'
    ),
    model = fct_reorder(model, .x = overall),
    pred_corr_max = factor(pred_corr_max),
    text = table_glue(
      "{model}<br>\\
          Mean computation time: {as.numeric(time)} seconds<br>\\
          C-statistic by variable class:<br>\\
          Main effect: {main}<br>\\
          Non-linear effect: {nlin}<br>\\
          Combination effect: {cmbn}<br>\\
          Interactions:<br>\\
             w/main effect: {intr_main}<br>\\
             w/non-linear effect: {intr_hidden_nlin}<br>\\
             w/combination effect: {intr_hidden_cmbn}"
    ))

fig <- plot_ly(width = 800, height = 650) %>%
  add_trace(
    type = 'bar',
    x = fig_data$pred_corr_max,
    y = fig_data$overall,
    color = fig_data$model,
    colors = "PuRd",
    text = table_value(fig_data$overall),
    textposition = 'outside',
    hoverinfo = 'text',
    hovertext = fig_data$text
  ) %>%
  plotly::layout(
    title = list(text = 'Results when N = 5000', x = 0.1),
    xaxis = list(title = 'Max correlation among predictors'),
    yaxis = list(title = 'C-statistic', range = c(0,1)),
    legend = list(title = list(text = '<b>VI technique</b>'))
  )

fig


```

---


## Using the accelerated oblique RSF

(https://bcjaeger.github.io/aorsf/)

```{r out.width='100%'}

knitr::include_url("https://bcjaeger.github.io/aorsf/")

```

---

# Thank you!

---
class: center, middle

# .huge[.rainbow-text[BONUS ROUND]]

---

# Data sets

A total of 31 risk prediction tasks in 18 data sets were analyzed.

This table is continued on the next 3 slides.

```{r}

tbls <- data_key |> 
  select(label, outcome, nrow, ncol, nevent, pcens, pmiss, pctns) |> 
  mutate(
    across(starts_with('p'),  ~.x * 100),
    across(where(is.numeric), .fns = table_value),
    outcome = Hmisc::capitalize(outcome)
  ) |> 
  arrange(label) |> 
  mutate(grp = seq(n()),
         grp = case_when(
           grp > 3 * n()/4 ~ 4,
           grp > 2 * n()/4 ~ 3, 
           grp > 1 * n()/4 ~ 2,
           TRUE ~ 1
         )) |> 
  group_by(grp) |> 
  group_map(
    ~ .x |> 
      gt(rowname_col = 'label') |> 
      cols_label(
        outcome = 'Outcome',
        nrow = 'N Obs',
        ncol = 'N Predictors',
        nevent = 'N Events',
        pcens = '% Censored',
        pmiss = '% Missing',
        pctns = "% Continuous"
      ) |> 
      cols_align(align = 'center') |> 
      cols_align(align = 'left', columns = 'outcome')
  )

tbls[[1]]

```

---
## Data sets continued

```{r}

tbls[[2]]

```

---

## Data sets continued

```{r}

tbls[[3]]

```

---

## Data sets continued

```{r}

tbls[[4]]

```

---

## Evaluation (bonus round)

Consider a testing data set:
$$\mathcal{D}_{\text{test}} = \left\{ (T_i, \delta_i, x_{i}) \right\}_{i=1}^{N_{\text{test}}}.$$
Let $\widehat{S}(t \mid x_i)$ be the predicted probability of survival up to a given prediction horizon of $t > 0$. 
The Brier score at time $t$ is $$\widehat{\text{BS}}(t) = \frac{1}{N_{\text{test}}} \sum_{i=1}^{N_{\text{test}}} \left\{ \widehat{S}(t \mid x_i)^2 \cdot I(T_i \leq t, \delta_i = 1) \cdot \widehat{G}(T_i)^{-1} + \\ [1-\widehat{S}(t \mid x_i)]^2 \cdot I(T_i > t) \cdot \widehat{G}(t)^{-1}\right\}$$ 

where $\widehat{G}(\cdot)$ is the Kaplan-Meier estimate of the censoring distribution. 

---

## Evaluation (bonus round)

As the Brier score is time dependent, integration over time provides a summary measure of performance over a range of plausible prediction horizons. The integrated Brier score is defined as 

$$\mathcal{\widehat{BS}}(t_a, t_b) = \frac{1}{t_b - t_a}\int_{t_a}^{t_b} \widehat{\text{BS}}(t) dt.$$ 
In our results 

- $t_a$ is the 25th percentile of event times

- $t_b$ is the 75th percentile of event times

---

## Evaluation (bonus round)

$\mathcal{\widehat{BS}}(t_a, t_b)$, a sum of squared prediction errors, can be scaled to produce a measure of explained residual variation (i.e., an $R^2$ statistic) by computing $$R^2 = 1 - \frac{\mathcal{\widehat{BS}}(t_a, t_b)}{\mathcal{\widehat{BS}}_0(t_a, t_b)}$$ where $\mathcal{\widehat{BS}}_0(t_a, t_b)$ is the integrated Brier score when a Kaplan-Meier estimate for survival based on the training data is used as the survival prediction function $\widehat{S}(t)$. 

_Jargon_: $R^2$ = __index of prediction accuracy__ (IPA) 



